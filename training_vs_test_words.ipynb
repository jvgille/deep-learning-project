{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"training_vs_test_words.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/jvgille/deep-learning-project/blob/master/training_vs_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0_Xm9E_cLIZS"},"source":["# Text Generator - Deep Learning project"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m92nftUxLYiJ"},"source":["## setup"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kafgUmAqI8cc","outputId":"affb43cb-75c9-446e-c85d-5c54389450cf","executionInfo":{"status":"ok","timestamp":1589739282068,"user_tz":-120,"elapsed":28084,"user":{"displayName":"Samuel Modée","photoUrl":"","userId":"02397843920135409332"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import math\n","import nltk\n","import itertools\n","nltk.download('punkt')\n","from collections import defaultdict\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 all, 1 no info, 2 no warning, 3 no error\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4LfnM5JoQUfM"},"source":["## Hyperparameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NIfDGL4GQXEk","colab":{}},"source":["file_location = '/content/drive/My Drive/DD2424 project/'\n","filename = 'shakespeare'\n","epochs = 50\n","batch_size = 64\n","seq_length = 100\n","rnn_units = 1024 # 1024\n","embedding_dim = 256 # not used atm\n","training_set_proportion = 0.99\n","shuffle_training_data = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6JU22eCmLzOT"},"source":["## Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g5WemSMoJBJR","colab":{}},"source":["def preprocess(filename, batch_size, seq_length, training_set_proportion, file_location='', shuffle_training_data=True):\n","    text = open(file_location + 'datasets/'+filename+'.txt', 'rb').read().decode(encoding='utf-8').splitlines()\n","    #text_length = len(text)\n","    #print(\"Text length: \", text_length)\n","    tt = [[t, \"\\n\"] for t in text]\n","    tt = list(itertools.chain.from_iterable(tt))\n","    tt = list(filter(None, tt))\n","    ll = []\n","    for t in tt:\n","        if t == \"\\n\":\n","            ll.append(t)\n","        else:\n","            dd = t.split()\n","            for d in dd:\n","                for w in nltk.word_tokenize(d):\n","                    ll.append(w)\n","                ll.append(' ')\n","\n","    text_length = len(ll)\n","    print(\"Number of words: \", text_length)\n","\n","    #temp = open(file_location + 'datasets/'+filename+'.txt', 'rb').read().decode(encoding='utf-8').split()\n","    #temp2 = []\n","    #for w in temp:\n","    #    t = ''.join(e for e in w if e.isalnum())\n","    #    if not t == \"\":\n","    #        temp2.append(t)\n","    \n","    print(\"Average word length: \", sum(len(w) for w in ll) / len(ll))\n","\n","    training_text = ll[:int((training_set_proportion * text_length))]\n","    test_text = ll[int((training_set_proportion * text_length)):]\n","\n","    print ('{} training_set unique words'.format(len(sorted(set(training_text)))))\n","    print ('{} test_set unique words'.format(len(sorted(set(test_text)))))\n","    print ('{} total number of unique words'.format(len(sorted(set(ll)))))\n","\n","    training_vocab = sorted(set(training_text))\n","\n","    char_to_idx = defaultdict(lambda:-1)\n","    idx_to_char = {-1: \"Unknown\"}\n","    for i, u in enumerate(training_vocab):\n","        char_to_idx[u] = i\n","        idx_to_char[i] = u\n","\n","    training_text_as_int = np.array([char_to_idx[c] for c in training_text])\n","    #test_text_as_int = np.array([char_to_idx[c] for c in test_text])\n","\n","    training_char_dataset = tf.data.Dataset.from_tensor_slices(training_text_as_int)\n","    #test_char_dataset = tf.data.Dataset.from_tensor_slices(test_text_as_int)\n","\n","    training_sequences = training_char_dataset.batch(seq_length+1, drop_remainder=True)\n","    #test_sequences = test_char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","    def split_input_target(chunk):\n","        input_text = chunk[:-1]\n","        target_text = chunk[1:]\n","        return input_text, target_text\n","\n","    training_dataset = training_sequences.map(split_input_target)\n","    #test_dataset = test_sequences.map(split_input_target)\n","\n","    BUFFER_SIZE = 10000 # TODO: make actual shuffle?\n","    if shuffle_training_data:\n","        training_dataset = training_dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n","\n","    \n","    return training_dataset, idx_to_char, char_to_idx, training_vocab, test_text, training_text\n","\n","def train_model(model, dataset, epochs, checkpoint_prefix):\n","    def loss(labels, logits):\n","        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","    model.compile(optimizer='adam', loss=loss)\n","\n","    # todo save best only\n","    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_prefix,\n","        save_weights_only=True,\n","        save_best_only=True,\n","        monitor='loss') # TODO monitor val_loss instead\n","\n","    return model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])\n","\n","\n","def one_hot(x):\n","    global vocab_size\n","    return tf.one_hot(tf.cast(x, 'uint8'), depth=vocab_size)\n","\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Lambda(one_hot, batch_input_shape=[batch_size,None]),\n","            tf.keras.layers.LSTM(rnn_units,\n","                                return_sequences=True,\n","                                stateful=True,\n","                                recurrent_initializer='glorot_uniform'),\n","            tf.keras.layers.Dense(vocab_size)\n","        ])\n","        return model\n","\n","\n","    \n","def generate_text(model, char_to_idx, idx_to_char,\n","                  start_string, num_generate=1000, temperature=1.0):\n","    # Converting our start string to numbers (vectorizing)\n","    input_eval = [char_to_idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    text_generated = []\n","\n","    model.reset_states()\n","    BPC_sum = 0\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # used for BPC/perplexity calculations. predictions is unnormalized log probabilities (that is how tf.random.cathegorical treats them later)\n","        current_prediction = tf.nn.softmax(predictions[-1, :]) # now we have normalized regular probabilities. -1 because we only care about the last output character/word\n","\n","        # using a categorical distribution to predict the character returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        current_probability = current_prediction[predicted_id]\n","        BPC_sum -= tf.math.log(current_probability) / math.log(2) # aka add minus log2 of prob\n","\n","        # We pass the predicted character as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx_to_char[predicted_id])\n","\n","    BPC = BPC_sum / num_generate\n","    return ( (start_string + ''.join(text_generated)), BPC)\n","\n","\n","\n","def evaluate_model(model, test_text, char_to_idx, idx_to_char):\n","    test_text_as_int = [char_to_idx[s] for s in test_text]\n","    text_length = len(test_text)\n","    model.reset_states()\n","    BPC_sum = 0\n","    unknown_count = 0\n","    for i in range(text_length - 1):\n","        if i % 1000 == 0:\n","          print(i, \" characters evaluated\")\n","        token = test_text_as_int[i]\n","        next_token = test_text_as_int[i+1]\n","        if token == -1 or next_token == -1: # did not exist in training\n","            unknown_count += 1\n","            model.reset_states()\n","            continue\n","            # could solve this instead by making sure that all characters in the test set are in the training set, works for characters in many situations\n","            \n","        input_eval = tf.expand_dims([token], 0)\n","        unnormalized_log_predictions = model(input_eval)\n","        next_token_probability = tf.nn.softmax(unnormalized_log_predictions[0, 0])[next_token]\n","        BPC_sum -= tf.math.log(next_token_probability) / math.log(2)\n","    \n","    BPW = BPC_sum / (text_length - 1 - unknown_count)\n","    BPC = BPW / 2.31  #Average word length for Shakespeare is 4.199964470061092 (without spaces)\n","                      #Average word length for Shakespeare is 2.3082163966064897 (with spaces)\n","    # TODO: calculate perplexity using BPC and average word length\n","    all_words = test_text  # also counts things like \",\" or \"!\" as words\n","    perplexity = 2 ** (2.31 * BPC)\n","    print(unknown_count)\n","\n","    return BPC, perplexity\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-Vp4wMjIMMg3"},"source":["## Test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ciR1wdpfLyjg","outputId":"27443f5b-e222-4b52-d71c-f696111dfd0b","executionInfo":{"status":"ok","timestamp":1589739322477,"user_tz":-120,"elapsed":22165,"user":{"displayName":"Samuel Modée","photoUrl":"","userId":"02397843920135409332"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["training_dataset, idx_to_char, char_to_idx, vocab, test_text, training_text = preprocess(filename=filename, \n","                                                      batch_size=batch_size, \n","                                                      seq_length=seq_length, \n","                                                      file_location=file_location, \n","                                                      training_set_proportion=training_set_proportion,\n","                                                      shuffle_training_data=shuffle_training_data)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Number of words:  497420\n","Average word length:  2.3082163966064897\n","14124 training_set unique words\n","778 test_set unique words\n","14210 total number of unique words\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IGfH_kHhMb7R","colab":{}},"source":["vocab_size = len(vocab)\n","checkpoint_dir = './training_checkpoints/' + filename\n","checkpoint_prefix = checkpoint_dir + '/ckpt_words'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xcx8rYKeRTPS","colab":{}},"source":["### Train model (comment out if only generating)\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JyL0IZPtRWOH","outputId":"3dc3974a-1db7-4e8d-fde2-2b619575f3d1","executionInfo":{"status":"ok","timestamp":1589742759861,"user_tz":-120,"elapsed":3428360,"user":{"displayName":"Samuel Modée","photoUrl":"","userId":"02397843920135409332"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # use this to continue training where it left off\n","history = train_model(model, training_dataset, epochs=epochs, checkpoint_prefix=checkpoint_prefix)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","76/76 [==============================] - 66s 868ms/step - loss: 4.7281\n","Epoch 2/50\n","76/76 [==============================] - 66s 866ms/step - loss: 4.3085\n","Epoch 3/50\n","76/76 [==============================] - 67s 878ms/step - loss: 4.1805\n","Epoch 4/50\n","76/76 [==============================] - 67s 881ms/step - loss: 3.8739\n","Epoch 5/50\n","76/76 [==============================] - 67s 880ms/step - loss: 3.5980\n","Epoch 6/50\n","76/76 [==============================] - 67s 881ms/step - loss: 3.4135\n","Epoch 7/50\n","76/76 [==============================] - 67s 883ms/step - loss: 3.3007\n","Epoch 8/50\n","76/76 [==============================] - 67s 881ms/step - loss: 3.2114\n","Epoch 9/50\n","76/76 [==============================] - 67s 882ms/step - loss: 3.1455\n","Epoch 10/50\n","76/76 [==============================] - 67s 882ms/step - loss: 3.0949\n","Epoch 11/50\n","76/76 [==============================] - 67s 883ms/step - loss: 3.0527\n","Epoch 12/50\n","76/76 [==============================] - 67s 882ms/step - loss: 3.0123\n","Epoch 13/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.9706\n","Epoch 14/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.9346\n","Epoch 15/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.8995\n","Epoch 16/50\n","76/76 [==============================] - 67s 882ms/step - loss: 2.8670\n","Epoch 17/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.8390\n","Epoch 18/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.8125\n","Epoch 19/50\n","76/76 [==============================] - 68s 890ms/step - loss: 2.7887\n","Epoch 20/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.7628\n","Epoch 21/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.7407\n","Epoch 22/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.7186\n","Epoch 23/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.6942\n","Epoch 24/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.6740\n","Epoch 25/50\n","76/76 [==============================] - 67s 887ms/step - loss: 2.6521\n","Epoch 26/50\n","76/76 [==============================] - 67s 887ms/step - loss: 2.6313\n","Epoch 27/50\n","76/76 [==============================] - 68s 894ms/step - loss: 2.6102\n","Epoch 28/50\n","76/76 [==============================] - 67s 887ms/step - loss: 2.5894\n","Epoch 29/50\n","76/76 [==============================] - 67s 887ms/step - loss: 2.5647\n","Epoch 30/50\n","76/76 [==============================] - 67s 888ms/step - loss: 2.5439\n","Epoch 31/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.5225\n","Epoch 32/50\n","76/76 [==============================] - 67s 883ms/step - loss: 2.5004\n","Epoch 33/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.4784\n","Epoch 34/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.4601\n","Epoch 35/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.4364\n","Epoch 36/50\n","76/76 [==============================] - 67s 886ms/step - loss: 2.4143\n","Epoch 37/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.3923\n","Epoch 38/50\n","76/76 [==============================] - 68s 891ms/step - loss: 2.3697\n","Epoch 39/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.3478\n","Epoch 40/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.3260\n","Epoch 41/50\n","76/76 [==============================] - 68s 894ms/step - loss: 2.3065\n","Epoch 42/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.2839\n","Epoch 43/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.2622\n","Epoch 44/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.2398\n","Epoch 45/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.2182\n","Epoch 46/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.1983\n","Epoch 47/50\n","76/76 [==============================] - 67s 885ms/step - loss: 2.1761\n","Epoch 48/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.1548\n","Epoch 49/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.1348\n","Epoch 50/50\n","76/76 [==============================] - 67s 884ms/step - loss: 2.1148\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p4X_HJpGRX20","outputId":"ee9fc375-ddcb-4ea4-a0fc-dc64fdf74cab","executionInfo":{"status":"ok","timestamp":1589743075465,"user_tz":-120,"elapsed":57117,"user":{"displayName":"Samuel Modée","photoUrl":"","userId":"02397843920135409332"}},"colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["### Generate sample (comment out if only training)\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))\n","\n","(output, BPC) = generate_text(model,char_to_idx,idx_to_char,\n","                              start_string=\"ANTONIO\", num_generate=500)\n","\n","print(\"BPC: \", BPC)\n","print(output)\n","\n","### Test BCP, perplexity\n","BPC, perplexity = evaluate_model(model=model, \n","               test_text=test_text, \n","               char_to_idx=char_to_idx, \n","               idx_to_char=idx_to_char)\n","print(\"BPC: \", BPC)\n","print(\"perplexity: \", perplexity)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["BPC:  tf.Tensor(3.621574, shape=(), dtype=float32)\n","ANTONIO thus and to a maid, \n","Nor they have made apparel inOur my person with brawling and vow you between \n","My thick cast what bid us devise need more. \n","\n","KING EDWARD IV: \n","to myself, inWho here my mother to hold: \n","The widow sorrow'd. \n","\n","CAMILLO: \n","Break it along, on him: \n","And not me all you Edward yeWhich daughter o make all voice \n","To hear them rebellion my death. \n","\n","GREMIO: \n","It re. \n","But thou deliver'd upon the market-place did. \n","\n","GLOUCESTER: \n","Wert feeling  further good true \n",", out that want have done deliver \n","Will, so and \n","A  , punishes' the commission of your authority tale \n","Re-quicken, and 't secretnot bring two not brawl. \n","Ah thou not laugh in grace king \n","From when he should; for, inThat bite, three-quarters indigested looks unseen. \n","\n","HENRY BOLINGBROKE: \n","to-morrow for that Edward are worth, \n","Tradition Richard left, these heads, and. \n","Let dead knee hearing that good soldiers for Richard! \n","But here, earth, or being thousand preparation he wiped, \n","'T first. \n","\n","MENENIUS: \n","O too, a cruel pretty'd a man! \n","\n","First Murderer: \n","Here's i, the boy, sir. \n","\n","First Murderer: \n","We general is eleven, my son's grown is: \n","are inhis charge thou do lay have done me \n","deliver the remembrance unto of a \n","0  characters evaluated\n","1000  characters evaluated\n","2000  characters evaluated\n","3000  characters evaluated\n","4000  characters evaluated\n","198\n","BPC:  tf.Tensor(2.163636, shape=(), dtype=float32)\n","perplexity:  tf.Tensor(31.95565, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ieo3CM2wqsuy","colab_type":"code","outputId":"d1634ab2-d13a-4f87-bffa-c783f432712b","executionInfo":{"status":"ok","timestamp":1589735184556,"user_tz":-120,"elapsed":6499,"user":{"displayName":"Samuel Modée","photoUrl":"","userId":"02397843920135409332"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["### Training perplexity\n","\n","BPC, perplexity = evaluate_model(model=model, \n","               test_text=training_text[:1000], \n","               char_to_idx=char_to_idx, \n","               idx_to_char=idx_to_char)\n","print(\"BPC: \", BPC)\n","print(\"perplexity: \", perplexity)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['First', ' ', 'Citizen', ':', ' ', '\\n', 'Before', ' ', 'we', ' ', 'proceed', ' ', 'any', ' ', 'further', ',', ' ', 'hear', ' ', 'me']\n","0  characters evaluated\n","BPC:  tf.Tensor(1.2079425, shape=(), dtype=float32)\n","perplexity:  tf.Tensor(33.669212, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sEqcmXtPRqbE","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]}]}